{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1H8ySAIpNYdbvuAY-Or7Mg8DLdQFa_g0x","authorship_tag":"ABX9TyMqr3FMoKhGSfh+f6U5DPzv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install hdf5storage"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ptM-dMYZwYH","executionInfo":{"status":"ok","timestamp":1715050258109,"user_tz":240,"elapsed":18874,"user":{"displayName":"Anuj Patel","userId":"05497770910676577379"}},"outputId":"3639bbb2-2c28-47c0-b013-a75b540a5f52"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hdf5storage\n","  Downloading hdf5storage-0.1.19-py2.py3-none-any.whl (53 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/53.6 kB\u001b[0m \u001b[31m612.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m808.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: h5py>=2.1 in /usr/local/lib/python3.10/dist-packages (from hdf5storage) (3.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hdf5storage) (1.25.2)\n","Installing collected packages: hdf5storage\n","Successfully installed hdf5storage-0.1.19\n"]}]},{"cell_type":"code","source":["!pip install -U scikit-fuzzy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h45cjmBBZ5Y-","executionInfo":{"status":"ok","timestamp":1715050273668,"user_tz":240,"elapsed":15563,"user":{"displayName":"Anuj Patel","userId":"05497770910676577379"}},"outputId":"0109ec0c-2542-438a-fd8a-d938bd6cca96"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-fuzzy\n","  Downloading scikit-fuzzy-0.4.2.tar.gz (993 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-fuzzy) (1.25.2)\n","Requirement already satisfied: scipy>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from scikit-fuzzy) (1.11.4)\n","Requirement already satisfied: networkx>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from scikit-fuzzy) (3.3)\n","Building wheels for collected packages: scikit-fuzzy\n","  Building wheel for scikit-fuzzy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for scikit-fuzzy: filename=scikit_fuzzy-0.4.2-py3-none-any.whl size=894078 sha256=33dfbad2a4560f50fd50bf9f463f8cae69cc7b5137b0dddac30f53218032dda3\n","  Stored in directory: /root/.cache/pip/wheels/4f/86/1b/dfd97134a2c8313e519bcebd95d3fedc7be7944db022094bc8\n","Successfully built scikit-fuzzy\n","Installing collected packages: scikit-fuzzy\n","Successfully installed scikit-fuzzy-0.4.2\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"lnkpbyL7SRuL","executionInfo":{"status":"ok","timestamp":1715050279469,"user_tz":240,"elapsed":5821,"user":{"displayName":"Anuj Patel","userId":"05497770910676577379"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","from tempfile import TemporaryFile\n","import scipy\n","import scipy.misc\n","from scipy.io import loadmat, savemat\n","import datetime\n","import h5py\n","import hdf5storage\n","import skfuzzy as fuzz\n","from glob import glob\n","import  random\n","import time\n"]},{"cell_type":"code","source":["config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","tf.compat.v1.enable_eager_execution(config=config)\n","layers = tf.keras.layers"],"metadata":{"id":"mb_9yG0HTWhc","executionInfo":{"status":"ok","timestamp":1715050279470,"user_tz":240,"elapsed":6,"user":{"displayName":"Anuj Patel","userId":"05497770910676577379"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Project_DL_WC/cGAN_python/generated_img\")"],"metadata":{"id":"FloDy7Fmcx8U","executionInfo":{"status":"error","timestamp":1715050311574,"user_tz":240,"elapsed":155,"user":{"displayName":"Anuj Patel","userId":"05497770910676577379"}},"colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"3c88d0ec-f452-46fd-e52e-5bf056400e1c"},"execution_count":12,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Project_DL_WC/cGAN_python/generated_img'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-2c11781583c1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/Project_DL_WC/cGAN_python/generated_img\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Project_DL_WC/cGAN_python/generated_img'"]}]},{"cell_type":"code","source":["def load_image_train(path, snr, batch_size=1):\n","    \"\"\"load, jitter, and normalize\"\"\"\n","    with h5py.File(path, 'r') as file:\n","        real_image = np.transpose(np.array(file[f'output_da_snr_{snr}']))\n","\n","    with h5py.File(path, 'r') as file:\n","        input_image = np.transpose(np.array(file[f'input_da_snr_{snr}']))\n","\n","    SIZE_IN = real_image.shape\n","    list_im = list(range(0, SIZE_IN[0]))\n","\n","    batch_im = random.sample(list_im, SIZE_IN[0])\n","    real_image = real_image[batch_im, :, :, :]\n","    input_image = input_image[batch_im, :, :, :]\n","\n","    n_batches = int(SIZE_IN[0] / batch_size)\n","\n","    for i in range(n_batches - 1):\n","        imgs_A = real_image[i * batch_size:(i + 1) * batch_size]\n","        imgs_B = input_image[i * batch_size:(i + 1) * batch_size]\n","\n","        yield imgs_A, imgs_B\n","\n","\n","def load_image_test(path, snr, batch_size=1):\n","    with h5py.File(path, 'r') as file:\n","        real_image = np.transpose(np.array(file[f'output_da_test_snr_{snr}']))\n","\n","    with h5py.File(path, 'r') as file:\n","        input_image = np.transpose(np.array(file[f'input_da_test_snr_{snr}']))\n","\n","    SIZE_IN = real_image.shape\n","\n","    n_batches = int(SIZE_IN[0] / batch_size)\n","\n","    for i in range(n_batches - 1):\n","        imgs_A = real_image[i * batch_size:(i + 1) * batch_size]\n","        imgs_B = input_image[i * batch_size:(i + 1) * batch_size]\n","\n","        yield imgs_A, imgs_B\n","\n","\n","def load_image_test_y(path, snr):\n","    with h5py.File(path, 'r') as file:\n","        real_image = np.transpose(np.array(file[f'output_da_test_snr_{snr}']))\n","\n","    with h5py.File(path, 'r') as file:\n","        input_image = np.transpose(np.array(file[f'input_da_test_snr_{snr}']))\n","\n","    return real_image, input_image"],"metadata":{"id":"dUfPgmTsOJ2z","executionInfo":{"status":"ok","timestamp":1715050461173,"user_tz":240,"elapsed":254,"user":{"displayName":"Anuj Patel","userId":"05497770910676577379"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","The architecture of generator is a modified U-Net.\n","There are skip connections between the encoder and decoder (as in U-Net).\n","\"\"\"\n","\n","\n","class EncoderLayer(tf.keras.Model):\n","    def __init__(self, filters, kernel_size, strides_s = 2, apply_batchnorm=True, add = False, padding_s = 'same'):\n","        super(EncoderLayer, self).__init__()\n","        initializer = tf.random_normal_initializer(mean=0., stddev=0.02)\n","        conv = layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides_s,\n","                             padding=padding_s, kernel_initializer=initializer, use_bias=False)\n","        ac = layers.LeakyReLU()\n","        self.encoder_layer = None\n","        if add:\n","            self.encoder_layer = tf.keras.Sequential([conv])\n","        elif apply_batchnorm:\n","            bn = layers.BatchNormalization()\n","            self.encoder_layer = tf.keras.Sequential([conv, bn, ac])\n","        else:\n","            self.encoder_layer = tf.keras.Sequential([conv, ac])\n","\n","    def call(self, x):\n","        return self.encoder_layer(x)\n","\n","\n","class DecoderLayer(tf.keras.Model):\n","    def __init__(self, filters, kernel_size, strides_s = 2, apply_dropout=False, add = False):\n","        super(DecoderLayer, self).__init__()\n","        initializer = tf.random_normal_initializer(mean=0., stddev=0.02)\n","        dconv = layers.Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides_s,\n","                                       padding='same', kernel_initializer=initializer, use_bias=False)\n","        bn = layers.BatchNormalization()\n","        ac = layers.ReLU()\n","        self.decoder_layer = None\n","\n","        if add:\n","            self.decoder_layer = tf.keras.Sequential([dconv])\n","        elif apply_dropout:\n","            drop = layers.Dropout(rate=0.5)\n","            self.decoder_layer = tf.keras.Sequential([dconv, bn, drop, ac])\n","        else:\n","            self.decoder_layer = tf.keras.Sequential([dconv, bn, ac])\n","\n","\n","\n","\n","    def call(self, x):\n","        return self.decoder_layer(x)\n","\n","\n","\n","\n","class Generator(tf.keras.Model):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        # Resize Input\n","        p_layer_1 = DecoderLayer(filters=2, kernel_size=4, strides_s = 2, apply_dropout=False, add = True)\n","        p_layer_2  = DecoderLayer(filters=2, kernel_size=4, strides_s = 2, apply_dropout=False, add = True)\n","        p_layer_3  = EncoderLayer(filters=2, kernel_size=(6,1),strides_s = (4,1), apply_batchnorm=False, add = True)\n","\n","        self.p_layers = [p_layer_1,p_layer_2,p_layer_3]\n","\n","\n","\n","        #encoder\n","        encoder_layer_1 = EncoderLayer(filters=64*1,  kernel_size=4,apply_batchnorm=False)\n","        encoder_layer_2 = EncoderLayer(filters=64*2, kernel_size=4)\n","        encoder_layer_3 = EncoderLayer(filters=64*4, kernel_size=4)\n","        encoder_layer_4 = EncoderLayer(filters=64*8, kernel_size=4)\n","        encoder_layer_5 = EncoderLayer(filters=64*8, kernel_size=4)\n","        self.encoder_layers = [encoder_layer_1, encoder_layer_2, encoder_layer_3, encoder_layer_4,\n","                               encoder_layer_5]\n","\n","        # decoder\n","        decoder_layer_1 = DecoderLayer(filters=64*8, kernel_size=4, apply_dropout=True)\n","        decoder_layer_2 = DecoderLayer(filters=64*8, kernel_size=4,apply_dropout=True)\n","        decoder_layer_3 = DecoderLayer(filters=64*8, kernel_size=4, apply_dropout=True)\n","        decoder_layer_4 = DecoderLayer(filters=64*4, kernel_size=4)\n","        self.decoder_layers = [decoder_layer_1, decoder_layer_2, decoder_layer_3, decoder_layer_4]\n","\n","\n","        initializer = tf.random_normal_initializer(mean=0., stddev=0.02)\n","        self.last = layers.Conv2DTranspose(filters=2, kernel_size=4, strides=2, padding='same',\n","                                           kernel_initializer=initializer, activation='tanh')\n","\n","    def call(self, x):\n","        # pass the encoder and record xs\n","        for p_layer in self.p_layers:\n","            x = p_layer(x)\n","\n","        encoder_xs = []\n","        for encoder_layer in self.encoder_layers:\n","            x = encoder_layer(x)\n","            encoder_xs.append(x)\n","        encoder_xs = encoder_xs[:-1][::-1]    # reverse\n","        assert len(encoder_xs) == 4\n","\n","        # pass the decoder and apply skip connection\n","        for i, decoder_layer in enumerate(self.decoder_layers):\n","            x = decoder_layer(x)\n","            x = tf.concat([x, encoder_xs[i]], axis=-1)     # skip connect\n","\n","        return self.last(x)        # last\n","\n"],"metadata":{"id":"tMjoZi4sTWkD","executionInfo":{"status":"ok","timestamp":1715050461750,"user_tz":240,"elapsed":293,"user":{"displayName":"Anuj Patel","userId":"05497770910676577379"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","The Discriminator is a PatchGAN.\n","\"\"\"\n","\n","\n","class Discriminator(tf.keras.Model):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        initializer = tf.random_normal_initializer(0., 0.02)\n","        # downsample\n","        self.encoder_layer_1 = EncoderLayer(filters=64, kernel_size=4, apply_batchnorm=False)\n","        self.encoder_layer_2 = EncoderLayer(filters=128, kernel_size=4)\n","        self.encoder_layer_3 = EncoderLayer(filters=128, kernel_size=4)\n","\n","        # conv block1\n","        self.zero_pad1 = layers.ZeroPadding2D()\n","        self.conv = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)\n","        self.bn1 = layers.BatchNormalization()\n","        self.ac = layers.LeakyReLU()\n","\n","        # block2\n","        self.zero_pad2 = tf.keras.layers.ZeroPadding2D()\n","        self.last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)\n","\n","    def call(self, y):\n","        \"\"\"inputs can be generated image. \"\"\"\n","        target = y\n","        x = target\n","        x = self.encoder_layer_1(x)\n","        x = self.encoder_layer_2(x)\n","        x = self.encoder_layer_3(x)\n","\n","        x = self.zero_pad1(x)\n","        x = self.conv(x)\n","        x = self.bn1(x)\n","        x = self.ac(x)\n","\n","        x = self.zero_pad2(x)\n","        x = self.last(x)\n","        return x"],"metadata":{"id":"0mwZoScJTWem","executionInfo":{"status":"ok","timestamp":1715050462035,"user_tz":240,"elapsed":8,"user":{"displayName":"Anuj Patel","userId":"05497770910676577379"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# GPU Setting\n","os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","# data path\n","path = \"/content/drive/MyDrive/Colab Notebooks/Project_DL_WC/Data_Generation_matlab/Gan_Data/Gan_0_dBIndoor2p4_64ant_32users_8pilot.mat\"\n","\n","\n","# batch = 1 produces good results on U-NET\n","BATCH_SIZE = 1\n","\n","# model\n","generator = Generator()\n","discriminator = Discriminator()\n","# optimizer\n","generator_optimizer = tf.compat.v1.train.AdamOptimizer(2e-4, beta1=0.5)\n","discriminator_optimizer = tf.compat.v1.train.RMSPropOptimizer(2e-5)\n","#discriminator_optimizer = tf.compat.v1.train.AdamOptimizer(2e-4, beta1=0.5)\n","\n"],"metadata":{"id":"WE_uhqzES-dh","executionInfo":{"status":"ok","timestamp":1715050462035,"user_tz":240,"elapsed":7,"user":{"displayName":"Anuj Patel","userId":"05497770910676577379"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["Discriminator loss:\n","The discriminator loss function takes 2 inputs; real images, generated images\n","real_loss is a sigmoid cross entropy loss of the real images and an array of ones(since the real images)\n","generated_loss is a sigmoid cross entropy loss of the generated images and an array of zeros(since the fake images)\n","Then the total_loss is the sum of real_loss and the generated_loss\n","\n","Generator loss:\n","It is a sigmoid cross entropy loss of the generated images and an array of ones.\n","The paper also includes L2 loss between the generated image and the target image.\n","This allows the generated image to become structurally similar to the target image.\n","The formula to calculate the total generator loss = gan_loss + LAMBDA * l2_loss, where LAMBDA = 100.\n","This value was decided by the authors of the paper."],"metadata":{"id":"lJ30L4zaV4_J"}},{"cell_type":"code","source":["def discriminator_loss(disc_real_output, disc_generated_output):\n","    \"\"\"disc_real_output = [real_target]\n","       disc_generated_output = [generated_target]\n","    \"\"\"\n","    real_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n","            labels=tf.ones_like(disc_real_output), logits=disc_real_output)  # label=1\n","    generated_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n","            labels=tf.zeros_like(disc_generated_output), logits=disc_generated_output)  # label=0\n","    total_disc_loss = tf.reduce_mean(real_loss) + tf.reduce_mean(generated_loss)\n","    return total_disc_loss\n","\n","\n","def generator_loss(disc_generated_output, gen_output, target, l2_weight=100):\n","    \"\"\"\n","        disc_generated_output: output of Discriminator when input is from Generator\n","        gen_output:  output of Generator (i.e., estimated H)\n","        target:  target image\n","        l2_weight: weight of L2 loss\n","    \"\"\"\n","    # GAN loss\n","    gen_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n","            labels=tf.ones_like(disc_generated_output), logits=disc_generated_output)\n","    # L2 loss\n","    l2_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","    total_gen_loss = tf.reduce_mean(gen_loss) + l2_weight * l2_loss\n","    return total_gen_loss\n","\n","\n","def generated_image(model, test_input, tar, t=0):\n","    \"\"\"Dispaly  the results of Generator\"\"\"\n","    prediction = model(test_input)\n","    #plt.figure(figsize=(15, 15))\n","    display_list = [np.squeeze(test_input[:,:,:,0]), np.squeeze(tar[:,:,:,0]), np.squeeze(prediction[:,:,:,0])]\n","\n","\n","    title = ['Input Y', 'Target H', 'Prediction H']\n","\n","    for i in range(3):\n","        plt.subplot(1, 3, i+1)\n","        plt.title(title[i])\n","        plt.imshow(display_list[i])\n","        plt.axis(\"off\")\n","        path = \"/content/drive/MyDrive/Colab Notebooks/Project_DL_WC/cGAN_python\"\n","        # print(path)\n","    plt.savefig(os.path.join(path,\"generated_img\", \"img_\"+str(t)+\".png\"))\n","\n","\n","\n","def train_step(input_image, target):\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        gen_output = generator(input_image)                      # input -> generated_target\n","        disc_real_output = discriminator(target)  # [input, target] -> disc output\n","        disc_generated_output = discriminator(gen_output)  # [input, generated_target] -> disc output\n","        # print(\"*\", gen_output.shape, disc_real_output.shape, disc_generated_output.shape)\n","\n","        # calculate loss\n","        gen_loss = generator_loss(disc_generated_output, gen_output, target)   # gen loss\n","        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)  # disc loss\n","\n","    # gradient\n","    generator_gradient = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    discriminator_gradient = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","    # apply gradient\n","    generator_optimizer.apply_gradients(zip(generator_gradient, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(discriminator_gradient, discriminator.trainable_variables))\n","    return gen_loss, disc_loss\n","\n","\n","def train(epochs, snr):\n","    nm = []\n","    ep = []\n","    start_time = datetime.datetime.now()\n","    for epoch in range(epochs):\n","        print(\"-----\\nEPOCH:\", epoch)\n","        # train\n","        for bi, (target, input_image) in enumerate(load_image_train(path, snr)):\n","            elapsed_time = datetime.datetime.now() - start_time\n","            gen_loss, disc_loss = train_step(input_image, target)\n","            if (bi % 103 == 0):\n","                print(\"B/E:\", bi, '/', epoch, \", Generator loss:\", gen_loss.numpy(), \", Discriminator loss:\",\n","                      disc_loss.numpy(), ', time:', elapsed_time)\n","        # generated and see the progress\n","        for bii, (tar, inp) in enumerate(load_image_test(path, snr)):\n","            if bii == 100:\n","                generated_image(generator, inp, tar, t=epoch + 1)\n","\n","        # save checkpoint\n","        # if (epoch + 1) % 2 == 0:\n","        ep.append(epoch + 1)\n","        # generator.save_weights(os.path.join(BASE_PATH, \"weights/generator_\"+str(epoch)+\".h5\"))\n","        # discriminator.save_weights(os.path.join(BASE_PATH, \"weights/discriminator_\"+str(epoch)+\".h5\"))\n","\n","        realim, inpuim = load_image_test_y(path, snr)\n","        prediction = generator(inpuim)\n","\n","        nm.append(fuzz.nmse(np.squeeze(realim), np.squeeze(prediction)))\n","\n","        if epoch == epochs - 1:\n","            nmse_epoch = TemporaryFile()\n","            np.save(nmse_epoch, nm)\n","\n","        # Save the predicted Channel\n","        matfiledata = {}  # make a dictionary to store the MAT data in\n","        matfiledata[u'predict_Gan_0_dB_Indoor2p4_64ant_32users_8pilot'] = np.array(prediction)\n","        # *** u prefix for variable name = unicode format, no issues thru Python 3.5; advise keeping u prefix indicator format based on feedback despite docs ***\n","        hdf5storage.write(matfiledata, '.', f'Results\\Eest_cGAN_{epoch + 1}_0db_Indoor2p4_64ant_32users_8pilot.mat',\n","                          matlab_compatible=True)\n","\n","        plt.figure()\n","        plt.plot(ep, nm, '^-r')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('NMSE')\n","        plt.show()\n","\n","    return nm, ep"],"metadata":{"id":"NNprxv0sVvXb","executionInfo":{"status":"ok","timestamp":1715050462036,"user_tz":240,"elapsed":8,"user":{"displayName":"Anuj Patel","userId":"05497770910676577379"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# if __name__ == \"__main__\":\n","\n","    # train\n","nm, ep = train(epochs=20, snr='0dB')\n","\n","plt.figure()\n","plt.plot(ep,nm,'^-r')\n","plt.xlabel('Epoch')\n","plt.ylabel('NMSE')\n","plt.show();\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"nIbtAmfnWEb_","executionInfo":{"status":"error","timestamp":1715050462036,"user_tz":240,"elapsed":7,"user":{"displayName":"Anuj Patel","userId":"05497770910676577379"}},"outputId":"b825fe86-4dea-4ca9-d4f7-999d18bf8015"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["-----\n","EPOCH: 0\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] Unable to open file (unable to open file: name = '/content/drive/MyDrive/Colab Notebooks/Project_DL_WC/Data_Generation_matlab/Gan_Data/Gan_0_dBIndoor2p4_64ant_32users_8pilot.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-294f0a04f7ee>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0dB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-baee4d409429>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, snr)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----\\nEPOCH:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_image_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-fc961415a6c6>\u001b[0m in \u001b[0;36mload_image_train\u001b[0;34m(path, snr, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"load, jitter, and normalize\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mreal_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'output_da_snr_{snr}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/content/drive/MyDrive/Colab Notebooks/Project_DL_WC/Data_Generation_matlab/Gan_Data/Gan_0_dBIndoor2p4_64ant_32users_8pilot.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"]}]},{"cell_type":"code","source":[],"metadata":{"id":"t7M1M7G1c1RZ"},"execution_count":null,"outputs":[]}]}